#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
speech_watcher.py  (2025-06-23 æ”¹è¨‚ç‰ˆ)

ãƒ‡ã‚¸ã‚¿ãƒ«åºã€Œå¤§è‡£ç­‰ä¼šè¦‹ã€ãƒšãƒ¼ã‚¸ã‹ã‚‰éå»4æ—¥ä»¥å†…ã®ä¼šè¦‹ã‚’æŠ½å‡ºã—ã€
å„ä¼šè¦‹ãƒšãƒ¼ã‚¸ã«åŸ‹ã‚è¾¼ã¾ã‚ŒãŸ YouTube å‹•ç”»ã®ãƒªãƒ³ã‚¯ã¨å†ç”Ÿæ™‚é–“ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚

å†ç”Ÿæ™‚é–“ãŒå–å¾—ã§ããªã„å ´åˆã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å‡ºåŠ›ã—ã€
ãã®ä¸‹ã«è©²å½“ã®ä¼šè¦‹ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚‚è¡¨ç¤ºã—ã¾ã™ã€‚
"""

import re
import time
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ å®šæ•°
JST = timezone(timedelta(hours=9))
TODAY = datetime.now(JST).replace(hour=0, minute=0, second=0, microsecond=0)
LOOKBACK_DAYS = 4
WINDOW_START = TODAY - timedelta(days=LOOKBACK_DAYS)

BASE_URL = "https://www.digital.go.jp"
LIST_URL = f"{BASE_URL}/speech"
UA = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36"
    )
}

REIWA_RE = re.compile(r"ä»¤å’Œ(\d+)å¹´(\d+)æœˆ(\d+)æ—¥")

def parse_iso8601_duration(duration: str) -> int:
    m = re.match(r'PT(?:(?P<h>\d+)H)?(?:(?P<m>\d+)M)?(?:(?P<s>\d+)S)?', duration)
    if not m:
        return 0
    h = int(m.group('h') or 0)
    mi = int(m.group('m') or 0)
    s = int(m.group('s') or 0)
    return h * 3600 + mi * 60 + s

def fetch_speech_items():
    resp = requests.get(LIST_URL, headers=UA, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    items = []
    for a in soup.select("a[href^='/speech/minister']"):
        text = a.get_text(" ", strip=True)
        m = REIWA_RE.search(text)
        if not m:
            continue
        era, month, day = map(int, m.groups())
        year = 2018 + era
        dt = datetime(year, month, day, tzinfo=JST)
        if not (WINDOW_START <= dt <= TODAY):
            continue

        title = re.sub(r"ï¼ˆ.*?ï¼‰", "", text)
        prefix = title[title.find("å¤§è‡£"):] if "å¤§è‡£" in title else title
        url = urljoin(BASE_URL, a["href"])
        items.append({"date": dt, "prefix": prefix, "page_url": url})
    return items

def lookup_youtube_in_speech(page_url: str):
    resp = requests.get(page_url, headers=UA, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    iframe = soup.find("iframe", src=re.compile(r"youtube\.com/embed/"))
    if iframe:
        src = iframe["src"]
        if src.startswith("//"):
            src = "https:" + src
        vid = src.rsplit("/", 1)[-1].split("?")[0]
    else:
        a = soup.find("a", href=re.compile(r"(youtu\.be/|youtube\.com/watch)"))
        if not a:
            return None, None
        href = a["href"]
        if "youtu.be/" in href:
            vid = href.split("youtu.be/")[1].split("?")[0]
        else:
            vid = href.split("v=")[1].split("&")[0]

    short_url = f"https://youtu.be/{vid}"
    watch_url = f"https://www.youtube.com/watch?v={vid}"
    r2 = requests.get(watch_url, headers=UA, timeout=10)
    r2.raise_for_status()
    meta = BeautifulSoup(r2.text, "html.parser").find("meta", itemprop="duration")
    if not meta or not meta.get("content"):
        return short_url, None
    total_sec = parse_iso8601_duration(meta["content"])
    return short_url, total_sec

def format_duration(sec: int) -> str:
    m, s = divmod(sec or 0, 60)
    return f"{m}åˆ†{s}ç§’"

def main():
    items = fetch_speech_items()
    if not items:
        print("è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—")
        return

    print("ã€å¹³å°†æ˜ãƒ‡ã‚¸ã‚¿ãƒ«å¤§è‡£ã€‘")
    for it in items:
        date_str = f"{it['date'].month}æœˆ{it['date'].day}æ—¥"
        prefix = it["prefix"]
        page_url = it["page_url"]
        yt_url, length = lookup_youtube_in_speech(page_url)

        if yt_url and length is not None:
            print(f"â—‹{date_str}ã®{prefix}ï¼ˆ{format_duration(length)}ï¼‰")
            print(f"ã€€{yt_url}")
        elif yt_url:
            print(f"â—‹{date_str}ã®{prefix}ï¼ˆå†ç”Ÿæ™‚é–“æƒ…å ±ã‚’è‡ªåˆ†ã§å–å¾—ã—ã¦ãã ã•ã„ï¼‰")
            print(f"ã€€{yt_url}")
            print(f"ã€€ï¼ˆä¼šè¦‹ãƒšãƒ¼ã‚¸ã‹ã‚‰è‡ªåˆ†ã§ç¢ºèªã—ã¦ï¼ï¼ï¼: {page_url}ï¼‰\n")
        else:
            print(f"â—‹{date_str}ã®{prefix}ï¼ˆï¼ï¼ï¼ï¼å†ç”Ÿæ™‚é–“æƒ…å ±ã‚’è‡ªåˆ†ã§å–å¾—ã—ã¦ãã ã•ã„ï¼ï¼ï¼ï¼ï¼ï¼ï¼‰")
            print(f"ã€€ï¼ˆä¼šè¦‹ãƒšãƒ¼ã‚¸ã‹ã‚‰è‡ªåˆ†ã§ç¢ºèªã—ã¦ï¼ï¼ï¼: {page_url}ï¼‰\n")

        time.sleep(0.2)

if __name__ == "__main__":
    main()

#============è‡ªæ°‘å…šï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ldp_watcher.py  revâ€‘4.6â€‘LDPâ€‘r4  (2025â€‘06â€‘17)

â–  è‡ªæ°‘å…šã‚µã‚¤ãƒˆï¼ˆ/activityï¼‰ã‚’å·¡å›ã—ï¼Œ
   éå» 4 æ—¥ï¼‹å½“æ—¥ï¼‹æœªæ¥ 10 æ—¥ã® 15 æ—¥åˆ†ã‹ã‚‰
   ãƒ‡ã‚¸ã‚¿ãƒ«æ”¿ç­–é–¢é€£ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿æŠ½å‡ºã—ã¦è¡¨ç¤ºã€‚
   â”€ é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œæœ¬æ–‡ãŒè©³ã—ã„æ–¹ã€ã‚’å„ªå…ˆã—ã¦ 1 è¡Œã«é›†ç´„ã€‚
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import re, time, sys, requests
from datetime import datetime, timezone, timedelta
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Global settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOOKBACK          = 4           # éå» 4 æ—¥
AHEAD             = 10          # æœªæ¥ 10 æ—¥
WAIT_SEC          = 1
DEBUG             = True
DEBUG_SOU         = True
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) "
      "Chrome/124.0.0.0 Safari/537.36")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORDS = [
    # â”€â”€ æ”¿æ²»ãƒ»æ”¿ç­– â”€â”€
    "ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šæ¨é€²æœ¬éƒ¨","çµŒæ¸ˆå®‰å…¨ä¿éšœå¯¾ç­–æœ¬éƒ¨","çµŒæ¸ˆå®‰å…¨ä¿éšœæ¨é€²æœ¬éƒ¨",
    "æƒ…å ±é€šä¿¡æˆ¦ç•¥èª¿æŸ»ä¼š","çµŒæ¸ˆæˆé•·æˆ¦ç•¥æœ¬éƒ¨","çŸ¥çš„è²¡ç”£æˆ¦ç•¥èª¿æŸ»ä¼š",
    "ç«¶äº‰æ”¿ç­–èª¿æŸ»ä¼š","ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚µãƒ¼ãƒ“ã‚¹","ç‰¹å®šåˆ©ç”¨è€…æƒ…å ±",
    "web3","web3.0ç ”ç©¶ä¼š","ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šæ§‹æƒ³ä¼šè­°","ãƒ‡ã‚¸ã‚¿ãƒ«è‡¨æ™‚è¡Œæ”¿èª¿æŸ»ä¼š",
    "ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šæ¨é€²ä¼šè­°",
    # â”€â”€ æŠ€è¡“ä¸€èˆ¬ â”€â”€
    "ãƒ‡ã‚¸ã‚¿ãƒ«","æƒ…å ±é€šä¿¡","ã‚µã‚¤ãƒãƒ¼","AI","ï¼¤ï¼¸","DX","IT","5g",
    # â”€â”€ è¡Œæ”¿é–¢é€£ â”€â”€
    "æ¨™æº–ä»•æ§˜","ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³","ç„¡ç·šå±€","å…è¨±çŠ¶","å…‰ãƒ•ã‚¡ã‚¤ãƒ",
    # â”€â”€ å¤§è‡£ä¼šè¦‹ â”€â”€
    "å¹³ãƒ‡ã‚¸ã‚¿ãƒ«å¤§è‡£",
]
SHORT_ASCII = {"ai", "it", "dx"}          # 2 æ–‡å­—è‹±èªã¯å˜èªå¢ƒç•Œã‚’æ„è­˜
norm  = lambda s: re.sub(r"\s+", "", s).lower()
def kw_hit(text: str) -> bool:
    t = norm(text)
    for k in KEYWORDS:
        kl = k.lower()
        if kl in SHORT_ASCII:
            if re.search(rf"(?:^|[^a-z0-9]){kl}(?:[^a-z0-9]|$)", t):
                return True
        elif kl in t:
            return True
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ—¥ä»˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JST   = timezone(timedelta(hours=9))
today = datetime.now(JST).replace(hour=0, minute=0, second=0, microsecond=0)

# éå» LOOKBACK æ—¥ ï½ å½“æ—¥ ï½ æœªæ¥ AHEAD æ—¥
DATES = [today - timedelta(days=delta)
         for delta in range(-AHEAD, LOOKBACK + 1)]

DATE_TAG = re.compile(r"(\d{4})-(\d{2})-(\d{2})")
DATE_TXT = re.compile(r"(\d{4})å¹´\s*0?(\d{1,2})æœˆ\s*0?(\d{1,2})æ—¥")
TRAIL_RE = re.compile(r"\s*(?:æ”¿ç­–|ä¼šè­°ç­‰|æ³•ä»¤|æ¡ç”¨)?\s*20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥$")

dbg  = lambda *m: print(*m, file=sys.stderr, flush=True) if DEBUG else None
sdbg = lambda *m: print("[SOU]", *m, file=sys.stderr, flush=True) if DEBUG_SOU else None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                         è‡ª æ°‘ å…š
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
HEAD_TAGS = ("dt", "h1", "h2", "h3", "h4", "li")
EXCLUDE_LDP = re.compile(r"^è¨˜è€…ä¼šè¦‹$")      # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰

def better(record_new, record_old):
    """ã©ã¡ã‚‰ã‚’æ®‹ã™ã‹åˆ¤å®šï¼ˆæœ¬æ–‡ãŒã‚¿ã‚¤ãƒˆãƒ«ã¨åŒã˜ãªã‚‰åŠ£ã‚‹ï¼‰"""
    body_n, body_o = record_new["body"], record_old["body"]
    ttl = record_new["title"]
    # æœ¬æ–‡ãŒç©º or ã‚¿ã‚¤ãƒˆãƒ«ã¨åŒã˜ â†’ æƒ…å ±é‡ 0
    score_n = len(body_n) if body_n and body_n != ttl else 0
    score_o = len(body_o) if body_o and body_o != ttl else 0
    return record_new if score_n > score_o else record_old

def scrape_ldp():
    # key=(æ—¥ä»˜, ã‚¿ã‚¤ãƒˆãƒ«) ã§æœ€è‰¯ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¿æŒ
    best = {}

    with sync_playwright() as p:
        ctx = (p.chromium
               .launch(headless=True,
                       args=["--disable-blink-features=AutomationControlled"])
               .new_context(user_agent=UA))
        page = ctx.new_page()

        for d in DATES:
            url = f"https://www.jimin.jp/activity/?day={d.year}.{d.month}.{d.day}"
            #dbg("[LDP] goto", url) <- ãƒ‡ãƒãƒƒã‚¯ã‚’è¦‹ãŸã‘ã‚Œã°ã“ã“ã‚’æœ‰åŠ¹åŒ–
            try:
                page.goto(url, wait_until="networkidle", timeout=25_000)
            except Exception:
                continue
            soup = BeautifulSoup(page.content(), "html.parser")

            for tag in soup.find_all(HEAD_TAGS):
                ttl = tag.get_text(" ", strip=True)
                if not ttl or EXCLUDE_LDP.match(ttl):
                    continue
                if not kw_hit(ttl):
                    continue
                sib = tag.find_next_sibling() or tag
                body = sib.get_text(" ", strip=True)
                if body.startswith("ä»Šæ—¥ã® è‡ªæ°‘å…š"):
                    body = ""

                rec = {
                    "date": f"{d.month}æœˆ{d.day}æ—¥",
                    "title": ttl,
                    "body": body.replace("Google Calenderã«äºˆå®šã‚’è¿½åŠ ", "").strip()
                }
                key = (rec["date"], rec["title"])
                if key in best:
                    best[key] = better(rec, best[key])
                else:
                    best[key] = rec
                #dbg(" ğŸ”¹LDP-HIT", ttl[:60])ã€€<- ãƒ‡ãƒãƒƒã‚¯ã‚’è¦‹ãŸã‘ã‚Œã°ã“ã“ã‚’æœ‰åŠ¹åŒ–

            time.sleep(WAIT_SEC)

    return list(best.values())

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    ldp = scrape_ldp()

    #print(f"\n===== {today.strftime('%-mæœˆ%-dæ—¥')} ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ =====\n")

    print("ã€è‡ªç”±æ°‘ä¸»å…šã€‘")
    if ldp:
        # æ–‡å­—åˆ—ã®æ—¥ä»˜ã‚’ä¸¦ã³æ›¿ãˆã‚„ã™ãæ•´æ•°åŒ–ã—ã¦ã‚½ãƒ¼ãƒˆ
        def dt_key(r):
            m, d = map(int, r["date"].rstrip("æ—¥").split("æœˆ"))
            return (m, d)
        for r in sorted(ldp, key=dt_key, reverse=False):
            print(f"â—‹{r['date']}ã€€{r['title']}")
            if r['body'] and r['body'] != r['title']:
                print(f"ã€€{r['body']}\n")
            else:
                print()          # æœ¬æ–‡ãŒç©ºã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«ã¨åŒã˜ãªã‚‰ 1 è¡Œã§
    else:
        print("DXã‚„ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã«é–¢é€£ã™ã‚‹æ–°ç€æƒ…å ±ãŠã‚ˆã³å¯©è­°ä¼šç­‰ã®é–‹å‚¬ã¯ã„ãšã‚Œã‚‚ãªã—\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    main()

#ãƒ‡ã‚¸ã‚¿ãƒ«åº
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
digital_watcher.py  rev-4.6-DIGITAL  (2025-06-23)

â–  å½¹å‰²
  ãƒ‡ã‚¸ã‚¿ãƒ«åºã‚µã‚¤ãƒˆã®ã€Œãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã€ã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã‹ã‚‰ã€
  ãƒ‡ã‚¸ã‚¿ãƒ«æ”¿ç­–é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã¿ã€ã‹ã¤ä¸€å®šæœŸé–“ (LOOKBACK/AHEAD) å†…ã«ç™ºä¿¡
  ã•ã‚ŒãŸè¨˜äº‹ã‚’æŠ½å‡ºã—ã¦ä¸€è¦§è¡¨ç¤ºã™ã‚‹ã€‚
"""
import re
import time
import unicodedata
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºæœ¬è¨­å®š
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36")
JST   = timezone(timedelta(hours=9))
TODAY = datetime.now(JST).replace(hour=0, minute=0, second=0, microsecond=0)

LOOKBACK = 5      # ä»Šæ—¥ + éå»4æ—¥
AHEAD    = 7      # æœªæ¥ (é–‹å‚¬æ¡ˆå†…ãªã©)
DIG_PAGES = 15    # å„ã‚«ãƒ†ã‚´ãƒªã§æ·±æ˜ã‚Šã™ã‚‹ãƒšãƒ¼ã‚¸æ•°
WAIT      = 0.3   # ç§’

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
RAW_KW = [
    # æŠ€è¡“ãƒ»è¡Œæ”¿ä¸€èˆ¬
    "ãƒ‡ã‚¸ã‚¿ãƒ«","æƒ…å ±é€šä¿¡","ã‚µã‚¤ãƒãƒ¼","AI","DX","ï¼¤ï¼¸","IT","SNS",
    "æ¨™æº–ä»•æ§˜","ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³","ç„¡ç·šå±€","å…è¨±çŠ¶","å…‰ãƒ•ã‚¡ã‚¤ãƒ",
]
SHORT = {"ai", "it", "dx"}

# å…¨è§’æ•°å­—â†’åŠè§’åŒ–ã—ã¦ NFKC æ­£è¦åŒ–ã€lower åŒ–
half = lambda s: ''.join(chr(ord(c)-0xFEE0) if 'ï¼' <= c <= 'ï¼™' else c for c in s)
norm_kw = [half(unicodedata.normalize("NFKC", k)).lower() for k in RAW_KW]

def kw_hit(txt: str) -> bool:
    t = half(unicodedata.normalize("NFKC", txt)).lower()
    return any(
        # çŸ­ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å˜èªå¢ƒç•Œã§ãƒãƒƒãƒãƒ³ã‚°
        (re.search(rf"(?:^|[^a-z0-9]){k}(?:[^a-z0-9]|$)", t) if k in SHORT else k in t)
        for k in norm_kw
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ—¥ä»˜åˆ¤å®š
WIN_FROM = TODAY - timedelta(days=LOOKBACK - 1)
WIN_TO   = TODAY + timedelta(days=AHEAD)
in_window = lambda d: WIN_FROM <= d <= WIN_TO

DIG_ROOT = ["https://www.digital.go.jp/press", "https://www.digital.go.jp/news"]
dt_re = re.compile(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥")

def article_date(html: str):
    """è¨˜äº‹è©³ç´° HTML ã‹ã‚‰ <time> ã¾ãŸã¯æœ¬æ–‡å†…ã®æ—¥ä»˜ã‚’å–å¾—"""
    soup = BeautifulSoup(html, "html.parser")
    # <time datetime="YYYY-MM-DD">
    if t := soup.find("time", datetime=True):
        y, m, d = map(int, t["datetime"][:10].split("-"))
        return datetime(y, m, d, tzinfo=JST)
    # æœ¬æ–‡ä¸­ã®ã€ŒYYYYå¹´MæœˆDæ—¥ã€
    if m := dt_re.search(soup.text):
        return datetime(*map(int, m.groups()), tzinfo=JST)

def scrape_digital():
    sess = requests.Session()
    sess.headers["User-Agent"] = UA
    hits, seen = [], set()

    for root in DIG_ROOT:
        for pg in range(1, DIG_PAGES + 1):
            url = root if pg == 1 else f"{root}?page={pg}"
            resp = sess.get(url, timeout=20)
            soup = BeautifulSoup(resp.text, "html.parser")
            page_has_hit = False

            for a in soup.select("a[href^='/press/'], a[href^='/news/']"):
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                title = a.get_text(" ", strip=True)
                # æœ«å°¾ã®ã€Œåˆ†é¡ ï¼‹ YYYYå¹´MæœˆDæ—¥ã€ã‚’å‰Šé™¤
                title = re.sub(r'\s+\S+\s+\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥$', '', title)

                if not title or not kw_hit(title):
                    continue

                link = urljoin(url, a["href"])
                if link in seen:
                    continue

                # è¨˜äº‹ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦æ—¥ä»˜ã‚’åˆ¤å®š
                art_html = sess.get(link, timeout=20).text
                dt = article_date(art_html)
                if not dt or not in_window(dt):
                    continue

                hits.append({
                    "date": dt.strftime("%-mæœˆ%-dæ—¥"),
                    "title": title,
                    "url": link
                })
                seen.add(link)
                page_has_hit = True

            # ãƒ’ãƒƒãƒˆãŒç„¡ã„ãƒšãƒ¼ã‚¸ã§ãƒ«ãƒ¼ãƒ—çµ‚äº†
            if not page_has_hit:
                break

            time.sleep(WAIT)

    return hits

def main():
    #print(f"===== Digitalåº Policy Watch ({TODAY:%-m/%-d}) =====\n")
    print("ã€ãƒ‡ã‚¸ã‚¿ãƒ«åºã€‘")
    results = scrape_digital()
    if not results:
        print("DXã‚„ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã«é–¢é€£ã™ã‚‹æ–°ç€æƒ…å ±ãŠã‚ˆã³å¯©è­°ä¼šç­‰ã®é–‹å‚¬ã¯ã„ãšã‚Œã‚‚ãªã—\n")
        return
    for r in results:
        print(f"âšªï¸{r['date']}ã€€{r['title']}\n{r['url']}\n")

if __name__ == "__main__":
    main()


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
soumu_watcher.py  revâ€‘4.6â€‘SOU  (2025â€‘06â€‘12)

â–  å½¹å‰²
  ç·å‹™çœã‚µã‚¤ãƒˆã€ŒWhat's Newã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èµ°æŸ»ã—ã€
  ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ»æƒ…å ±é€šä¿¡æ”¿ç­–ã«é–¢ã™ã‚‹å‘ŠçŸ¥ã®ã†ã¡ã€LOOKBACKã€œAHEAD æœŸé–“ã«
  è©²å½“ã™ã‚‹ã‚‚ã®ã‚’æŠ½å‡ºã—ã¦ä¸€è¦§è¡¨ç¤ºã™ã‚‹ã€‚
"""
import re, unicodedata
import requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºæœ¬è¨­å®š
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36")
JST   = timezone(timedelta(hours=9))
TODAY = datetime.now(JST).replace(hour=0, minute=0, second=0, microsecond=0)

LOOKBACK = 5      # ä»Šæ—¥ + éå»4æ—¥
AHEAD    = 7      # æœªæ¥ (é–‹å‚¬æ¡ˆå†…ãªã©)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
RAW_KW = [
    # æŠ€è¡“ãƒ»è¡Œæ”¿ä¸€èˆ¬
    "ãƒ‡ã‚¸ã‚¿ãƒ«","æƒ…å ±é€šä¿¡","ã‚µã‚¤ãƒãƒ¼","AI","DX","ï¼¤ï¼¸","IT","SNS",
    "ç„¡ç·šå±€","å…è¨±çŠ¶","å…‰ãƒ•ã‚¡ã‚¤ãƒ","æ¨™æº–ä»•æ§˜","ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
    # å¯©è­°ä¼šé–¢é€£
    "æƒ…å ±é€šä¿¡å¯©è­°ä¼š","éƒµæ”¿æ”¿ç­–éƒ¨ä¼š","é›»æ°—é€šä¿¡äº‹æ¥­éƒ¨ä¼š",
    "æŠ€è¡“åˆ†ç§‘ä¼š","é™¸ä¸Šç„¡ç·šé€šä¿¡å§”å“¡ä¼š","IPãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å‚™å§”å“¡ä¼š",
]
SHORT = {"ai", "it", "dx"}

half = lambda s: ''.join(chr(ord(c)-0xFEE0) if 'ï¼' <= c <= 'ï¼™' else c for c in s)
norm_kw = [half(unicodedata.normalize("NFKC", k)).lower() for k in RAW_KW]

def kw_hit(txt: str) -> bool:
    t = half(unicodedata.normalize("NFKC", txt)).lower()
    return any(
        re.search(rf"(?:^|[^a-z0-9]){k}(?:[^a-z0-9]|$)", t) if k in SHORT else k in t
        for k in norm_kw
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ—¥ä»˜è§£æ
jp_re  = re.compile(r"ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥")
ymd_re = re.compile(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥")
slash  = re.compile(r"(\d{4})/(\d{1,2})/(\d{1,2})")

def parse_dt(text: str):
    """ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯æœ¬æ–‡å†…ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã—ã¦ datetime ã«å¤‰æ›"""
    t = half(unicodedata.normalize("NFKC", text))
    if m := jp_re.search(t):
        return datetime(2018 + int(m[1]), int(m[2]), int(m[3]), tzinfo=JST)
    if m := ymd_re.search(t):
        return datetime(*map(int, m.groups()), tzinfo=JST)
    if m := slash.search(t):
        return datetime(*map(int, m.groups()), tzinfo=JST)

WIN_FROM = TODAY - timedelta(days=LOOKBACK - 1)
WIN_TO   = TODAY + timedelta(days=AHEAD)
in_window = lambda d: WIN_FROM <= d <= WIN_TO

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä½ãƒ¬ãƒ™ãƒ« fetchï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šï¼‰
def fetch(url):
    r = requests.get(url, headers={"User-Agent": UA}, timeout=25)
    enc = r.apparent_encoding or "utf-8"
    if enc.lower() == "utf-8" and b"\x82" in r.content[:300]:   # SJISèª¤åˆ¤å®šå¯¾ç­–
        enc = "shift_jis"
    return r.content.decode(enc, "replace")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ What's New ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å€™è£œæŠ½å‡º
def list_candidates():
    idx = "https://www.soumu.go.jp/menu_kyotsuu/whatsnew/index.html"
    with sync_playwright() as p:
        page = p.chromium.launch(headless=True).new_context().new_page()
        page.goto(idx, wait_until="networkidle", timeout=30000)
        soup = BeautifulSoup(page.content(), "html.parser")

    links = []
    for a in soup.find_all("a"):
        ttl = a.get_text(" ", strip=True)
        if ttl and kw_hit(ttl):
            links.append({"title": ttl, "url": urljoin(idx, a["href"])})
    return links

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç·å‹™çœã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—
def scrape_soumu():
    results = []
    for rec in list_candidates():
        try:
            html = fetch(rec["url"])
        except Exception:
            continue
        dt = parse_dt(rec["title"]) or parse_dt(html)
        if not dt or not in_window(dt):
            continue
        results.append({"date": dt.strftime("%-mæœˆ%-dæ—¥"), **rec})

    # é‡è¤‡æ’é™¤
    uniq, filtered = set(), []
    for r in results:
        key = (r["date"], r["title"])
        if key in uniq:
            continue
        uniq.add(key)
        filtered.append(r)
    return filtered

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
def main():
    #print(f"===== ç·å‹™çœ What's New Watch ({TODAY:%-m/%-d}) =====\n")
    print("ã€ç·å‹™çœã€‘")
    results = scrape_soumu()
    if not results:
        print("DXã‚„ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã«é–¢é€£ã™ã‚‹æ–°ç€æƒ…å ±ãŠã‚ˆã³å¯©è­°ä¼šç­‰ã®é–‹å‚¬ã¯ã„ãšã‚Œã‚‚ãªã—")
        return
    for r in results:
        print(f"â—‹{r['date']}ã€€{r['title']}\nã€€{r['url']}\n")


if __name__ == "__main__":
    main()

print("ã€çµŒæ¸ˆç”£æ¥­çœã€‘")
print("è‡ªå‹•åŒ–ã§ããªã„ã®ã§æ‰‹å‹•ã§èª¿ã¹ã¦ãã ã•ã„!!!!\n")

#å†…é–£åºœ    
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
cao_press_watcher_rss_etree.py  rev-1.6  (2025-06-19)

â–  å†…é–£åºœã€Œå ±é“ç™ºè¡¨æ–°ç€æƒ…å ±ã€RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã ã‘ã§å–å¾—ãƒ»è§£æã—ã€
  éå» 4 æ—¥é–“ã«æ²è¼‰ã•ã‚ŒãŸ â€œDXï¼ãƒ‡ã‚¸ã‚¿ãƒ«é–¢é€£ï¼‹é£Ÿå“ãƒ»ç’°å¢ƒâ€ ã®
  ãƒªãƒªãƒ¼ã‚¹ã‚’æŠ½å‡ºã—ã¦ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚

ãƒ»requests ã§ RSS(XML) ã‚’å–å¾—
ãƒ»xml.etree.ElementTree ã§ãƒ‘ãƒ¼ã‚¹
ãƒ»email.utils.parsedate_to_datetime + datetime.fromisoformat ã§æ—¥ä»˜å¤‰æ›
ä¾å­˜:
    pip install requests
"""

import re
import unicodedata
import requests

from datetime import datetime, timedelta, timezone
from xml.etree import ElementTree as ET
from email.utils import parsedate_to_datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RSS_URL       = "https://www.cao.go.jp/rss/news.rdf"
LOOKBACK_DAYS = 4

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Date window â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JST      = timezone(timedelta(hours=9))
NOW      = datetime.now(JST)
TODAY    = NOW.replace(hour=0, minute=0, second=0, microsecond=0)
WIN_FROM = TODAY - timedelta(days=LOOKBACK_DAYS)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Keywords â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORDS = [
    "ç’°å¢ƒ",
    "DX", "ãƒ‡ã‚¸ã‚¿ãƒ«", "ã‚¯ãƒ©ã‚¦ãƒ‰", "ã‚¬ãƒãƒ¡ãƒ³ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼",
    "çµŒæ¸ˆå®‰å…¨ä¿éšœ", "QUAD", "ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¯ãƒªã‚¢ãƒ©ãƒ³ã‚¹",
    "é›»æ°—é€šä¿¡äº‹æ¥­æ³•", "ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "Web3", "åŠå°ä½“", "AI",
    "GIGAã‚¹ã‚¯ãƒ¼ãƒ«æ§‹æƒ³", "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼", "ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿",
    "ã‚¹ãƒãƒ›æ–°æ³•", "é’å°‘å¹´ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆç’°å¢ƒæ•´å‚™æ³•", "Fintech",
    "ä¸­å¤®éŠ€è¡Œãƒ‡ã‚¸ã‚¿ãƒ«é€šè²¨", "çŸ¥çš„è²¡ç”£", "å€‹äººæƒ…å ±ä¿è­·", "åŒ»ç™‚DX",
    "æ–°å¹´åº¦äºˆç®—ï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«é–¢é€£ï¼‰"
]
SHORT_ASCII = {"ai", "it", "dx"}

def normalize(text: str) -> str:
    return unicodedata.normalize("NFKC", text).lower()

def kw_hit(text: str) -> bool:
    t = normalize(text)
    for kw in KEYWORDS:
        k = normalize(kw)
        if k in SHORT_ASCII:
            if re.search(rf"(?:^|[^a-z0-9]){k}(?:[^a-z0-9]|$)", t):
                return True
        elif k in t:
            return True
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Fetch RSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_rss(url: str) -> str:
    resp = requests.get(url, timeout=(10, 30))
    resp.raise_for_status()
    return resp.text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Parse and filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_cao_rss():
    xml = fetch_rss(RSS_URL)
    root = ET.fromstring(xml)

    # define namespaces
    ns = {
        'rss': 'http://purl.org/rss/1.0/',
        'dc':  'http://purl.org/dc/elements/1.1/'
    }

    items = root.findall('rss:item', ns)

    results = []
    for itm in items:
        title_el = itm.find('rss:title', ns)
        link_el  = itm.find('rss:link',  ns)
        date_el  = itm.find('dc:date',   ns)
        if title_el is None or link_el is None or date_el is None:
            continue

        title = title_el.text.strip()
        link  = link_el.text.strip()
        date_text = date_el.text.strip()

        # parse RFC822 or ISO8601 date
        dt = None
        try:
            dt = parsedate_to_datetime(date_text)
        except Exception:
            try:
                dt = datetime.fromisoformat(date_text)
            except Exception:
                continue

        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=JST)
        dt_jst = dt.astimezone(JST)
        dt0    = dt_jst.replace(hour=0, minute=0, second=0, microsecond=0)

        # date window filter
        if not (WIN_FROM <= dt0 <= TODAY):
            continue

        # keyword filter
        if not kw_hit(title):
            continue

        results.append({
            'dt':   dt0,
            'date': dt0.strftime('%-mæœˆ%-dæ—¥'),
            'title': title,
            'url':   link
        })

    # dedupe & sort descending
    seen = set()
    out = []
    for r in sorted(results, key=lambda x: x['dt'], reverse=True):
        key = (r['date'], r['title'])
        if key in seen:
            continue
        seen.add(key)
        out.append(r)

    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    recs = scrape_cao_rss()
    print("ã€å†…é–£åºœã€‘")
    if not recs:
        print("DXã‚„ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã«é–¢é€£ã™ã‚‹æ–°ç€æƒ…å ±ãŠã‚ˆã³å¯©è­°ä¼šç­‰ã®é–‹å‚¬ã¯ã„ãšã‚Œã‚‚ãªã—\n")
        return
    for r in recs:
        print(f"â—‹{r['date']}ã€€{r['title']}\n")
        print(f"ã€€{r['url']}\n")

if __name__ == "__main__":
    main()

#ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼NISCãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import re
import unicodedata
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def to_ascii(s: str) -> str:
    """
    å…¨è§’æ•°å­—ãªã©ã‚’åŠè§’ã«æ­£è¦åŒ–ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼
    """
    return unicodedata.normalize('NFKC', s)

def fetch_recent_nisc_news(days: int = 4):
    BASE_URL = 'https://www.nisc.go.jp'
    # æŠ½å‡ºå¯¾è±¡ã¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    KEYWORDS = [
        "ãƒ‡ã‚¸ã‚¿ãƒ«", "æƒ…å ±é€šä¿¡", "ã‚µã‚¤ãƒãƒ¼", "AI", "DX", "ï¼¤ï¼¸", "IT", "SNS",
        "æ¨™æº–ä»•æ§˜", "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³", "ç„¡ç·šå±€", "å…è¨±çŠ¶", "å…‰ãƒ•ã‚¡ã‚¤ãƒ"
    ]

    today     = datetime.now()
    threshold = today - timedelta(days=days)
    results   = []

    # â€”â€” ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› â€”â€”
    #print(f'DEBUG: today     = {today.strftime("%Y-%m-%d")}')
    #print(f'DEBUG: threshold = {threshold.strftime("%Y-%m-%d")}')

    # é–¾å€¤ã€œä»Šæ—¥ã¾ã§ã®å„æ—¥ä»˜ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
    for delta in range((today - threshold).days + 1):
        dt  = threshold + timedelta(days=delta)
        url = f'{BASE_URL}/news/{dt.strftime("%Y%m%d")}.html'
        #print(f'DEBUG: checking URL = {url}')

        resp = requests.get(url)
        #print(f'DEBUG: raw status_code = {resp.status_code}')
        if resp.status_code != 200:
            continue

        # 1) ãƒã‚¤ãƒˆåˆ—ã§ãƒ‘ãƒ¼ã‚¹ã—ã¦<meta charset>ã‚’æ¢ã™
        soup_bytes = BeautifulSoup(resp.content, 'html.parser')
        meta_charset = soup_bytes.find('meta', attrs={'charset': True})
        if meta_charset:
            encoding = meta_charset['charset']
        else:
            encoding = resp.encoding or resp.apparent_encoding or 'utf-8'
        #print(f'DEBUG: detected encoding = {encoding}')

        # 2) æ­£ã—ã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ç›´ã—
        resp.encoding = encoding
        page_text = resp.text
        soup      = BeautifulSoup(page_text, 'html.parser')

        # 3) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
        full_text = soup.get_text()
        matched = any(kw in full_text for kw in KEYWORDS)
        #print(f'DEBUG: keyword match = {matched}')
        if not matched:
            continue

        # 4) ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        h2 = soup.find('h2')
        title = h2.get_text(strip=True) if h2 else '[ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜]'
        #print(f'DEBUG: title = {title}')

        # 5) ãƒšãƒ¼ã‚¸å†…ã‹ã‚‰å…¬é–‹æ—¥ã‚’ãƒ‘ãƒ¼ã‚¹
        raw_date = ''
        elem = soup.find(string=re.compile(r'\d{4}å¹´'))
        if elem:
            raw_date = elem.strip()
        #print(f'DEBUG: raw date_text = "{raw_date}"')

        ascii_date = to_ascii(raw_date)
        m = re.search(r'(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥', ascii_date)
        if m:
            y, mth, d = map(int, m.groups())
            dt_pub = datetime(y, mth, d)
            #print(f'DEBUG: parsed dt_pub = {dt_pub.strftime("%Y-%m-%d")}')
        else:
            # ã†ã¾ããƒ‘ãƒ¼ã‚¹ã§ããªã‘ã‚Œã° URL æ—¥ä»˜ã‚’ãã®ã¾ã¾ä½¿ã†
            dt_pub = dt
            #print(f'DEBUG: date parse failed, using URL date = {dt_pub.strftime("%Y-%m-%d")}')

        results.append((dt_pub, title, url))

    #print(f'DEBUG: total matched results = {len(results)}\n')

    # â€”â€” æœ€çµ‚å‡ºåŠ› â€”â€”
    print('ã€å†…é–£ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚»ãƒ³ã‚¿ãƒ¼ãƒ»NISCã€‘')
    if results:
        for dt_pub, title, url in sorted(results):
            print(f'â—‹{dt_pub.month}æœˆ{dt_pub.day}æ—¥ã€€ã€Œ{title}ã€')
            print(f'ã€€{url}\n')
    else:
        print(f'{threshold.month}æœˆ{threshold.day}æ—¥ã€œ{today.month}æœˆ{today.day}æ—¥ã€€'
              'DXã‚„ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã«é–¢é€£ã™ã‚‹æ–°ç€æƒ…å ±ãŠã‚ˆã³å¯©è­°ä¼šç­‰ã®é–‹å‚¬ã¯ã„ãšã‚Œã‚‚ãªã—\n')


if __name__ == '__main__':
    fetch_recent_nisc_news(4)

#-----------é‡‘èåºãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import re
import unicodedata
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def to_ascii(s: str) -> str:
    """
    å…¨è§’æ•°å­—ãªã©ã‚’åŠè§’ã«æ­£è¦åŒ–
    """
    return unicodedata.normalize('NFKC', s)

def fetch_fsa_news(days: int = 4):
    BASE_URL = 'https://www.fsa.go.jp'
    # æŠ½å‡ºå¯¾è±¡ã¨ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆäººäº‹ãƒ»äººäº‹ç•°å‹•ã‚‚è¿½åŠ ï¼‰
    KEYWORDS = [
        "ãƒ‡ã‚¸ã‚¿ãƒ«", "æƒ…å ±é€šä¿¡", "ã‚µã‚¤ãƒãƒ¼", "AI", "DX", "ï¼¤ï¼¸", "IT", "SNS",
        "æ¨™æº–ä»•æ§˜", "ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³", "ç„¡ç·šå±€", "å…è¨±çŠ¶", "å…‰ãƒ•ã‚¡ã‚¤ãƒ",
        "äººäº‹", "äººäº‹ç•°å‹•"
    ]

    today     = datetime.now()
    threshold = today - timedelta(days=days)
    results   = []

    # â€”â€” ãƒ‡ãƒãƒƒã‚°å‡ºåŠ› â€”â€”
    #print(f'DEBUG: today     = {today.strftime("%Y-%m-%d")}')
    #print(f'DEBUG: threshold = {threshold.strftime("%Y-%m-%d")}')

    # â‘  /inter/etc/YYYYMMDD/YYYYMMDD.html ã®ãƒ«ãƒ¼ãƒ—ãƒã‚§ãƒƒã‚¯
    for delta in range((today - threshold).days + 1):
        dt = threshold + timedelta(days=delta)
        subpath = f'/inter/etc/{dt.strftime("%Y%m%d")}/{dt.strftime("%Y%m%d")}.html'
        url     = BASE_URL + subpath
        #print(f'DEBUG: checking URL = {url}')

        resp = requests.get(url)
        #print(f'DEBUG: status_code = {resp.status_code}')
        if resp.status_code != 200:
            continue

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºï¼†å†ãƒ‡ã‚³ãƒ¼ãƒ‰
        soup_bytes   = BeautifulSoup(resp.content, 'html.parser')
        meta_charset = soup_bytes.find('meta', attrs={'charset': True})
        encoding = (meta_charset['charset']
                    if meta_charset
                    else resp.encoding or resp.apparent_encoding or 'utf-8')
        #print(f'DEBUG: detected encoding = {encoding}')
        resp.encoding = encoding
        soup = BeautifulSoup(resp.text, 'html.parser')

        # å…¬é–‹æ—¥ï¼ˆä»¤å’Œè¡¨è¨˜ï¼‰ã‚’æœ¬æ–‡ã‹ã‚‰ãƒ‘ãƒ¼ã‚¹
        raw_date = ''
        date_elem = soup.find(string=re.compile(r'ä»¤å’Œ'))
        if date_elem:
            raw_date = date_elem.strip()
        #print(f'DEBUG: raw_date = "{raw_date}"')

        m = re.search(r'ä»¤å’Œ(\d+)å¹´\s*(\d+)æœˆ\s*(\d+)æ—¥',
                      to_ascii(raw_date))
        if m:
            era, mth, d = map(int, m.groups())
            year = 2018 + era
            dt_pub = datetime(year, mth, d)
        else:
            dt_pub = dt
        #print(f'DEBUG: parsed dt_pub = {dt_pub.strftime("%Y-%m-%d")}')

        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        title_tag = soup.find('title') or soup.find('h1') or soup.find('h2')
        title     = title_tag.get_text(strip=True) if title_tag else '[ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜]'
        #print(f'DEBUG: title = {title}')

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæœ¬æ–‡å…¨ä½“ï¼‰
        full_text = soup.get_text()
        matched   = any(kw in full_text for kw in KEYWORDS)
        #print(f'DEBUG: keyword match = {matched}')
        if not matched:
            continue

        results.append((dt_pub, title, url))

    # â‘¡ äººäº‹ç•°å‹•ãƒšãƒ¼ã‚¸ ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ï¼‰
    j_url = BASE_URL + '/common/about/jinji/index.html'
    #print(f'DEBUG: checking HR URL = {j_url}')
    resp = requests.get(j_url)
    #print(f'DEBUG: status_code = {resp.status_code}')
    if resp.status_code == 200:
        soup_bytes   = BeautifulSoup(resp.content, 'html.parser')
        meta_charset = soup_bytes.find('meta', attrs={'charset': True})
        encoding = (meta_charset['charset']
                    if meta_charset
                    else resp.encoding or resp.apparent_encoding or 'utf-8')
        #print(f'DEBUG: detected encoding HR = {encoding}')
        resp.encoding = encoding
        soup = BeautifulSoup(resp.text, 'html.parser')

        full_text = soup.get_text()
        matched   = any(kw in full_text for kw in KEYWORDS)
        #print(f'DEBUG: HR keyword match = {matched}')
        if matched:
            # ç™ºä»¤æ—¥ã‚’ã™ã¹ã¦æŠ½å‡ºï¼†ç¯„å›²ãƒã‚§ãƒƒã‚¯
            text_ascii = to_ascii(full_text)
            hr_dates = re.findall(r'ä»¤å’Œ7å¹´\s*(\d+)æœˆ\s*(\d+)æ—¥ç™ºä»¤', text_ascii)
            #print(f'DEBUG: HR raw matches = {hr_dates}')
            for mth_str, day_str in hr_dates:
                mth, day = int(mth_str), int(day_str)
                dt_pub = datetime(2018 + 7, mth, day)
                in_range = threshold <= dt_pub <= today
                #print(f'DEBUG: HR dt_pub = {dt_pub.strftime("%Y-%m-%d")}, in_range = {in_range}')
                if in_range:
                    title = f'äººäº‹ç•°å‹•ï¼ˆä»¤å’Œï¼—å¹´{mth}æœˆ{day}æ—¥ä»˜ï¼‰ã«ã¤ã„ã¦å…¬è¡¨ã—ã¾ã—ãŸã€‚'
                    results.append((dt_pub, title, j_url))

    #print(f'DEBUG: total matched results = {len(results)}\n')

    # â€”â€” æœ€çµ‚å‡ºåŠ› â€”â€”
    print('ã€é‡‘èåºã€‘')
    if results:
        for dt_pub, title, url in sorted(results):
            print(f'â—‹{dt_pub.month}æœˆ{dt_pub.day}æ—¥ã€€ã€Œ{title}ã€')
            print(f'ã€€{url}\n')
    else:
        print(f'{threshold.month}æœˆ{threshold.day}æ—¥ã€œ{today.month}æœˆ{today.day}æ—¥ã€€'
              'DXã‚„ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã«é–¢é€£ã™ã‚‹æ–°ç€æƒ…å ±ãŠã‚ˆã³å¯©è­°ä¼šç­‰ã®é–‹å‚¬ã¯ã„ãšã‚Œã‚‚ãªã—\n')


if __name__ == '__main__':
    fetch_fsa_news(4)


#ãƒ‹ãƒ¥ãƒ¼ã‚¹    
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
gov_dx_news_scraper.py  rev-2.1  (2025-06-17)

â–  æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ Google News RSS ã‚’æ¤œç´¢ã—ã€
  çœåºãƒ»è‡ªæ²»ä½“ãŒé–¢ä¸ã™ã‚‹ DX é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿æŠ½å‡ºã€‚
  4æ—¥ã‚ˆã‚Šå¤ã„è¨˜äº‹ã¯é™¤å¤–ã—ã¾ã™ã€‚
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import re, sys, html, time, hashlib, unicodedata, requests, xml.etree.ElementTree as ET
from email.utils import parsedate_to_datetime
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
from urllib.parse import quote_plus

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORDS = [
    "DX","ãƒ‡ã‚¸ã‚¿ãƒ«","ã‚¯ãƒ©ã‚¦ãƒ‰","ã‚¬ãƒãƒ¡ãƒ³ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰","ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼",
    "çµŒæ¸ˆå®‰å…¨ä¿éšœ","QUAD","ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³","ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¯ãƒªã‚¢ãƒ©ãƒ³ã‚¹",
    "é›»æ°—é€šä¿¡äº‹æ¥­æ³•","ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£","Web3","åŠå°ä½“","AI",
    "GIGAã‚¹ã‚¯ãƒ¼ãƒ«æ§‹æƒ³","é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼","ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿",
    "ã‚¹ãƒãƒ›æ–°æ³•","é’å°‘å¹´ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆç’°å¢ƒæ•´å‚™æ³•","Fintech",
    "ä¸­å¤®éŠ€è¡Œãƒ‡ã‚¸ã‚¿ãƒ«é€šè²¨","çŸ¥çš„è²¡ç”£","å€‹äººæƒ…å ±ä¿è­·","åŒ»ç™‚DX",
    "æ–°å¹´åº¦äºˆç®— ãƒ‡ã‚¸ã‚¿ãƒ«","ã‚¹ãƒãƒ›ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç«¶äº‰ä¿ƒé€²æ³•","apple"
    "ã‚¢ãƒƒãƒ—ãƒ«","ã‚°ãƒ¼ã‚°ãƒ«","google","ç›¸äº’é‹ç”¨æ€§"
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¡Œæ”¿ä¸»ä½“ãƒ•ã‚£ãƒ«ã‚¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MINISTRIES = [
    "ç·å‹™çœ","çµŒæ¸ˆç”£æ¥­çœ","ãƒ‡ã‚¸ã‚¿ãƒ«åº","æ–‡éƒ¨ç§‘å­¦çœ","çµŒç”£çœ","åšç”ŸåŠ´åƒçœ",
    "è¾²æ—æ°´ç”£çœ","å›½åœŸäº¤é€šçœ","è²¡å‹™çœ","é‡‘èåº","ç’°å¢ƒçœ","å¤–å‹™çœ","é˜²è¡›çœ",
    "å†…é–£åºœ","å†…é–£å®˜æˆ¿","è­¦å¯Ÿåº","æ¶ˆé˜²åº","å¾©èˆˆåº","å…¬æ­£å–å¼•å§”å“¡ä¼š","å…¬å–å§”",
    "å›½äº¤çœ","åšåŠ´çœ","è¾²æ°´çœ","ãƒ‡ã‚¸åº","æ–‡ç§‘çœ","Google","ã‚¢ãƒƒãƒ—ãƒ«","apple",
]
PREF_SUFFIX = ("çœŒ","åºœ","éƒ½","å¸‚","ç”º","æ‘")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¤œç´¢è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36")
JST  = timezone(timedelta(hours=9))
SINCE_DAYS = 4  # â† ã“ã“ã‚’ 4 æ—¥ã«å¤‰æ›´
RSS_URL = "https://news.google.com/rss/search?hl=ja&gl=JP&ceid=JP:ja&q=" \
          "{}%20when:4d"  # â† when:14d ã‚’ when:4d ã«å¤‰æ›´

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_gov_related(text:str)->bool:
    if any(w in text for w in MINISTRIES):
        return True
    if re.search(r"(æ”¿åºœ|å†…é–£|è‡ªæ²»ä½“|å›½ãŒ|å›½ã¯)", text):
        return True
    for suf in PREF_SUFFIX:
        if re.search(rf"[^\w]{{1,4}}{suf}", text):
            return True
    return False

def strip_html(raw:str)->str:
    return BeautifulSoup(html.unescape(raw), "html.parser").get_text(" ", strip=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ RSS å–å¾— & è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_hits(keyword:str):
    url = RSS_URL.format(quote_plus(keyword))
    headers = {"User-Agent": UA}
    xml_data = requests.get(url, headers=headers, timeout=30).content

    root = ET.fromstring(xml_data)
    for item in root.iterfind(".//item"):
        title = strip_html(item.findtext("title", default=""))
        descr = strip_html(item.findtext("description", default=""))
        if not is_gov_related(title + descr):
            continue

        link = item.findtext("link", default="")
        try:
            dt = parsedate_to_datetime(item.findtext("pubDate", ""))
        except Exception:
            continue
        dt = dt.astimezone(JST)
        if dt < datetime.now(JST) - timedelta(days=SINCE_DAYS):
            continue

        yield {
            "dt": dt,
            "date": f"{dt.month}æœˆ{dt.day}æ—¥",
            "title": title,
            "url": link
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ãƒ¡ã‚¤ãƒ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    news, seen = [], set()
    for kw in KEYWORDS:
        try:
            for hit in fetch_hits(kw):
                uid = hashlib.md5(hit["url"].encode()).hexdigest()
                if uid in seen:
                    continue
                seen.add(uid)
                news.append(hit)
        except Exception as e:
            print(f"[WARN] {kw}: {e}", file=sys.stderr)
        time.sleep(0.6)

    news.sort(key=lambda x: x["dt"])

    print("ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘")
    if not news:
        print("è©²å½“è¨˜äº‹ãªã—")
        return
    for n in news:
    # ãƒ»6æœˆ9æ—¥ã€€ã‚¿ã‚¤ãƒˆãƒ«
        print(f"â—‹{n['date']}ã€€{n['title']}")
    # ã€€URL
        print(f"ã€€{n['url']}\n")


if __name__ == "__main__":
    main()
